{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mario_DeepRL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FCSkgbb1-UJ8",
        "gqlfdiRknwtl",
        "xr07pe40oNmB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73u40Lit0vZN"
      },
      "source": [
        "# Deep Reinforcement Learning Mario\n",
        "This notebook goes through implementing and comparing the performance of several Deep Reinforcement Learning agents playing Mario Super Smash Bros. It includes the DDQN and Dueling DDQN agents. \n",
        "\n",
        "A simple environment [CartPole](https://gym.openai.com/envs/CartPole-v0/) is used to verify the correctness of agent implementations. \n",
        "\n",
        "**Choose the environment:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgsO8XDyDWSb"
      },
      "source": [
        "# Choose one:\n",
        "mario_env_name = \"Mario\"\n",
        "cartpole_env_name = \"CartPole\"\n",
        "\n",
        "env_name = cartpole_env_name"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtvk_qD-DXg8"
      },
      "source": [
        "## First, dependencies\n",
        "Packages for rendering gym environment from the remote server where the notebook is executed. It will capture a video of the gym there and display it inline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bk0YefW9iEW"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5J32s8ykmBx"
      },
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuikPb8GRDTk",
        "outputId": "f3f6bd0f-731d-4541-b2f5-42f3635536fc"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1025'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1025'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5-ams6ajN2k"
      },
      "source": [
        "Gym packages including CartPole and Mario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UudtlqaXkewo"
      },
      "source": [
        "!pip install gym gym-super-mario-bros > /dev/null 2>&1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv01IOVr_hdw"
      },
      "source": [
        "# This cell is temporary. It is needed to get the DQN example from PyTorch to work\n",
        "\n",
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils > /dev/null 2>&1\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* > /dev/null 2>&1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkTp2dG4AtUh"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = display.start()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7atF3rRes283"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # only display errors\n",
        "\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnE606x2lnKi"
      },
      "source": [
        "Now, the actual computational packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n08L83tN9Ch"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3jrXIvmH9V"
      },
      "source": [
        "and some python utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb22eL_qmNCC"
      },
      "source": [
        "from collections import namedtuple\n",
        "from itertools import count"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSOcBffN4X4G"
      },
      "source": [
        "and mounting the drive for saving models and loading modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-mJBKgk9Gr",
        "outputId": "ac611168-ebd5-454c-80a0-2e6e0906612b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI2tOT4AAz5F",
        "outputId": "de7e4e4c-4ff6-456b-8ba4-7766b48bac59"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/Colab Notebooks') #This needs to be the location you place Prioritized Memory files at\n",
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data  video\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkjsOTcG4Vuq"
      },
      "source": [
        "and some other set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rOKlJS2-KMQ"
      },
      "source": [
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21qUO7Nnm_EF"
      },
      "source": [
        "##Environment Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uv_2agZREDp"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video(i=-1):\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  print(mp4list)\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[i]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY7Ks7BRnHoP"
      },
      "source": [
        "## Mario Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL46VHbqyrRe"
      },
      "source": [
        "if env_name == mario_env_name:\n",
        "    env = wrap_env(gym_super_mario_bros.make('SuperMarioBros-v2'))\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "    def get_screen():\n",
        "        # Images comes as HWC, transpose to CHW\n",
        "        screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "        # Normalize\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        # Add a batch dimension BCHW (required for convolutional layers)\n",
        "        return screen.unsqueeze(0).to(device)\n",
        "\n",
        "    env.reset()\n",
        "    plt.figure()\n",
        "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "            interpolation='none')\n",
        "    plt.title('Example extracted screen')\n",
        "    plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCSkgbb1-UJ8"
      },
      "source": [
        "## Cartpole Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "KTzfoqKnDA11",
        "outputId": "34c36385-a206-4351-ed97-0cea23a48a2f"
      },
      "source": [
        "if env_name == cartpole_env_name:\n",
        "    env = gym.make('CartPole-v0').unwrapped\n",
        "    env = wrap_env(env)\n",
        "    resize = T.Compose([T.ToPILImage(),\n",
        "                        T.Resize(40, interpolation=Image.CUBIC),\n",
        "                        T.ToTensor()])\n",
        "\n",
        "\n",
        "    def get_cart_location(screen_width):\n",
        "        world_width = env.x_threshold * 2\n",
        "        scale = screen_width / world_width\n",
        "        return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "    def get_screen():\n",
        "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "        screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "        _, screen_height, screen_width = screen.shape\n",
        "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "        view_width = int(screen_width * 0.6)\n",
        "        cart_location = get_cart_location(screen_width)\n",
        "        if cart_location < view_width // 2:\n",
        "            slice_range = slice(view_width)\n",
        "        elif cart_location > (screen_width - view_width // 2):\n",
        "            slice_range = slice(-view_width, None)\n",
        "        else:\n",
        "            slice_range = slice(cart_location - view_width // 2,\n",
        "                                cart_location + view_width // 2)\n",
        "        # Strip off the edges, so that we have a square image centered on a cart\n",
        "        screen = screen[:, :, slice_range]\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        # (this doesn't require a copy)\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        # Resize, and add a batch dimension (BCHW)\n",
        "        return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    env.reset()\n",
        "    plt.figure()\n",
        "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "            interpolation='none')\n",
        "    plt.title('Example extracted screen')\n",
        "    plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECLGALmnOuv"
      },
      "source": [
        "## Agent in parts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqlfdiRknwtl"
      },
      "source": [
        "### Replay Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIPGGvTn4Q9"
      },
      "source": [
        "from priority_memory import Memory\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class PrioritizedMemory(Memory):\n",
        "    def __init__(self, capacity, dqn):\n",
        "      super().__init__(capacity)\n",
        "      self.dqn = dqn\n",
        "\n",
        "    def push(self, *args):\n",
        "      transition = Transition(*args)\n",
        "      target = self.dqn.online_model(transition.state).data\n",
        "      old_val = target[0][transition.action]\n",
        "      if transition.done:\n",
        "          target[0][transition.action] = transition.reward.type(torch.float32)\n",
        "      else:\n",
        "          target_val = self.dqn.target_model(transition.next_state).data\n",
        "          target[0][transition.action] = transition.reward + GAMMA * torch.max(target_val)\n",
        "      error = abs(old_val - target[0][transition.action])\n",
        "      self.add(error.item(), transition)\n",
        "    # sample and len are defined in super class Memory\n",
        "\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr07pe40oNmB"
      },
      "source": [
        "###Models: DQN, Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdvhyjAeq4sy"
      },
      "source": [
        "# Helper for calculating kernel size after a convolution\n",
        "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "    return (size - (kernel_size - 1) - 1) // stride  + 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgAXPrJ6oMuF"
      },
      "source": [
        "####DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stDT2dENoitO"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    # h, w: height and width of the input image\n",
        "    # outputs: the number of outputs, corresponds to the size of the action space\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "\n",
        "        self.fc = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Returns a score for each action in the current state\n",
        "    # State x is assumed to be a 3 channel image\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x)\n",
        "        # Make each item in the batch a single row vec\n",
        "        return self.fc(conv_out.view(conv_out.size(0), -1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ83SFHbHYPP"
      },
      "source": [
        "#### Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqlnLRMHWKI"
      },
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "    # h, w: height and width of the input image\n",
        "    # outputs: the number of outputs, corresponds to the size of the action space\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.outputs = outputs\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "\n",
        "        # Multiple values corresponding to actions\n",
        "        self.advantage = nn.Sequential(\n",
        "            nn.Linear(linear_input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, outputs)\n",
        "        )\n",
        "\n",
        "        # A single value corresponding to the state\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(linear_input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    # Returns a score for each action in the current state\n",
        "    # State x is assumed to be a 3 channel image\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x)\n",
        "        # Make each item in the batch a single row vec\n",
        "        conv_out = conv_out.view(conv_out.size(0), -1)\n",
        "\n",
        "        value = self.value(conv_out).expand(x.size(0), self.outputs)\n",
        "        adv = self.advantage(conv_out)\n",
        "\n",
        "        # value = self.fc2_value(value).expand(x.size(0), self.outputs)\n",
        "\n",
        "        advAverage = adv.mean(1).unsqueeze(1).expand(x.size(0), self.outputs)\n",
        "        Q = value + adv - advAverage\n",
        "\n",
        "        return Q"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzF4rVzmArvc"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwaY2u5vAwwy"
      },
      "source": [
        "# Hyper params\n",
        "BATCH_SIZE = 128\n",
        "# Discount of future rewards\n",
        "GAMMA = 0.999\n",
        "# Epsilon greedy exploration\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 100 # 5000 good for 100k\n",
        "# Rate of target (frozen) model weight updates\n",
        "TARGET_UPDATE = 100\n",
        "MEMORY_SIZE = 10000"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Hl5i6SBBgM"
      },
      "source": [
        "# Computes threshold based on the current step\n",
        "def get_epsilon_threshold(step):\n",
        "    return EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * step / EPS_DECAY)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "MHZmSyGmBQ7w",
        "outputId": "c2c568ad-d131-4e85-cec7-cb6977e4fabe"
      },
      "source": [
        "# Show how epsion will decay\n",
        "steps = np.linspace(0, 1000)\n",
        "fig = plt.figure()\n",
        "plt.plot(steps, [get_epsilon_threshold(step) for step in steps])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f29b00e8588>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JAzciJJCp6u"
      },
      "source": [
        "### Agent Body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYUTRc96m06d"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, dueling_DQN = True, DDQN = True, PER = True):\n",
        "        # TODO: init from saved model weights like online_model.load_state_dict(torch.load(path))\n",
        "        # TODO: too much stuff in initializer?\n",
        "\n",
        "        # Maybe it shouldn't be here\n",
        "        self.env = env\n",
        "\n",
        "        _, _, h, w = get_screen().shape\n",
        "        # Target is the offline network that stays frozen and gets updated\n",
        "        # every TARGET_UPDATE\n",
        "        if dueling_DQN:\n",
        "            self.online_model = Dueling_DQN(h, w, env.action_space.n)\n",
        "            self.target_model = Dueling_DQN(h, w, env.action_space.n)\n",
        "        else:\n",
        "            self.online_model = DQN(h, w, env.action_space.n)\n",
        "            self.target_model = DQN(h, w, env.action_space.n)\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "        self.online_model = self.online_model.to(device)\n",
        "        self.target_model = self.target_model.to(device)\n",
        "            \n",
        "        self.optimizer = optim.RMSprop(self.online_model.parameters())\n",
        "\n",
        "        self.DDQN = DDQN\n",
        "        self.PER = PER\n",
        "\n",
        "        if self.PER:\n",
        "            self.memory = PrioritizedMemory(MEMORY_SIZE, self)\n",
        "        else:\n",
        "            self.memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "        # Keep track to adjust exploration (epsion) and update target model\n",
        "        self.steps_done = 0\n",
        "\n",
        "        # Save model weights at these intervals\n",
        "        self.save_interval = 1000\n",
        "\n",
        "    # Makes model good\n",
        "    def optimize_model(self):\n",
        "        # TODO: I assume PER is involved here as well\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        if self.PER:\n",
        "            transitions, idxs, is_weights = self.memory.sample(BATCH_SIZE)\n",
        "        else:\n",
        "            transitions = self.memory.sample(BATCH_SIZE)\n",
        "        # This converts batch-array of Transitions to Transition of batch-arrays.\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # Compute a mask of non-final states and concatenate the batch elements\n",
        "        # (a final state would've been the one after which simulation ended)\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        if self.DDQN:\n",
        "            # Compute: Reward + (Gamma * max_wrt_actions(Q_target(s_t+1, arg_max_wrt_actions(Q_online(s_t+1, a)))))\n",
        "            # 1. arg_max_wrt_actions(Q_online(s_t+1, a))\n",
        "            online_action_inds = self.online_model(non_final_next_states).argmax(1)\n",
        "            # 2. Q_target(s_t+1, all_actions)\n",
        "            target_all_action_q = self.target_model(non_final_next_states)\n",
        "            # 3. Q_target(s_t+1, arg_max_wrt_actions(Q_online(s_t+1, a)))\n",
        "            target_state_action_q = torch.zeros(BATCH_SIZE, device=device)\n",
        "            target_state_action_q[non_final_mask] = target_all_action_q.gather(1, online_action_inds.unsqueeze(-1))[0].detach()\n",
        "            # 4. Reward + (Gamma * max_wrt_actions(Q_target(s_t+1, arg_max_wrt_actions(Q_online(s_t+1, a)))))\n",
        "            target_state_action_q = (target_state_action_q * GAMMA) + reward_batch\n",
        "\n",
        "        else:\n",
        "            # Compute: Reward + (Gamma * max_wrt_actions(Q_target(s_t+1, a)))\n",
        "            # 1. max_wrt_actions(Q_target(s_t+1, a))\n",
        "            target_state_action_q = torch.zeros(BATCH_SIZE, device=device)\n",
        "            target_state_action_q[non_final_mask] = self.target_model(non_final_next_states).max(1)[0].detach()\n",
        "            # 2. Reward + (Gamma * max_wrt_actions(Q_target(s_t+1, a)))\n",
        "            target_state_action_q = (target_state_action_q * GAMMA) + reward_batch\n",
        "            \n",
        "        # Compute Q(s_t, a) and select Q values corresponding to the taken actions.\n",
        "        online_state_action_q = self.online_model(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute Huber loss\n",
        "        loss = F.smooth_l1_loss(online_state_action_q, target_state_action_q.unsqueeze(1))\n",
        "\n",
        "        #update prioritized memory\n",
        "        if self.PER:\n",
        "            errors = torch.abs(target_state_action_q.unsqueeze(1) - online_state_action_q).cpu().detach().numpy()\n",
        "            for i in range(BATCH_SIZE):\n",
        "                idx = idxs[i]\n",
        "                self.memory.update(idx, errors[i])\n",
        "            loss = (loss * torch.tensor(is_weights, dtype=torch.float32, device=device)).mean()\n",
        "        \n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.online_model.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    # Uses epsilon greedy with annealing: chances of choosing a random action are\n",
        "    # high in the beginning, but get lower further into training\n",
        "    # TODO: try reward based epsilon decay https://aakash94.github.io/Reward-Based-Epsilon-Decay/\n",
        "    def select_action(self, state):\n",
        "        self.steps_done += 1\n",
        "\n",
        "        if (self.steps_done % TARGET_UPDATE == 0):\n",
        "            self.update_target_model()\n",
        "        if (self.steps_done % self.save_interval == 0):\n",
        "            self.save_model()\n",
        "\n",
        "        if random.random() > get_epsilon_threshold(self.steps_done):\n",
        "            with torch.no_grad():\n",
        "                # Pick action with the largest expected reward\n",
        "                return self.online_model(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            # return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "            return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "    def save_model(self, model_name=\"Mario_RL.pt\"):\n",
        "        # TODO: don't overwrite models, save other info like graphs?\n",
        "        path = f\"/content/gdrive/My Drive/{model_name}\"\n",
        "        torch.save(self.online_model.state_dict(), path)\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "\n",
        "    def remember(self, transition):\n",
        "        # transition is: (state, action, next_state, reward, done)\n",
        "        self.memory.push(*transition)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySjIsO0kpZkV"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgEDWypppdzk"
      },
      "source": [
        "episode_durations = []\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration (steps)')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItBMjkRQtjFO"
      },
      "source": [
        "###Training loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRl75VGNtoQ7"
      },
      "source": [
        "Below, you can find the main training loop. At the beginning we reset\n",
        "the environment and initialize the ``state`` Tensor. Then, we sample\n",
        "an action, execute it, observe the next screen and the reward (always\n",
        "1), and optimize our model once. When the episode ends (our model\n",
        "fails), we restart the loop.\n",
        "\n",
        "Below, `num_episodes` is set small. You should download\n",
        "the notebook and run lot more epsiodes, such as 300+ for meaningful\n",
        "duration improvements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf0EDuFmVwvz"
      },
      "source": [
        "agent = Agent(env, False, False, True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "67Oj7wqEtrOb",
        "outputId": "20a615a1-c226-4b8e-8527-4d4c20e9968e"
      },
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    env.render()\n",
        "\n",
        "    if env_name == mario_env_name:\n",
        "        state = get_screen()\n",
        "    else:\n",
        "        # CartPole needs to capture motion\n",
        "        last_screen = get_screen()\n",
        "        current_screen = get_screen()\n",
        "        state = current_screen - last_screen\n",
        "\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "        # Give negative reward for losing the game\n",
        "        # TODO: works for cartpole but Mario??\n",
        "        reward = reward if not done or t == 499 else -10\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        if env_name == cartpole_env_name:\n",
        "            last_screen = current_screen\n",
        "            current_screen = get_screen()\n",
        "            if not done:\n",
        "                next_state = current_screen - last_screen\n",
        "            else:\n",
        "                next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.remember((state, action, next_state, reward, done))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        agent.optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "show_video()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-aa4b3d42e4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mepisode_durations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-9c8ebc9f1b42>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# This converts batch-array of Transitions to Transition of batch-arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Compute a mask of non-final states and concatenate the batch elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: zip argument #128 must support iteration"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}